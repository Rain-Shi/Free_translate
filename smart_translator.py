"""
æ™ºèƒ½æ–‡æ¡£ç¿»è¯‘ä¸æ ¼å¼ä¿çœŸç³»ç»Ÿ
åŸºäºåˆ›æ–°çš„æ··åˆç­–ç•¥ï¼šç»“æ„åˆ†å±‚è§£æ + è¯­ä¹‰å¢å¼ºç¿»è¯‘ + æ ¼å¼æ™ºèƒ½é‡å»º
"""

import streamlit as st
import tempfile
import os
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement, qn
import json
import re
from typing import Dict, List, Tuple, Any
import openai

class StructuralParser:
    """ç»“æ„åˆ†å±‚è§£æå™¨ - å°†æ–‡æ¡£åˆ†è§£ä¸ºå†…å®¹å±‚ã€æ ¼å¼å±‚ã€å¸ƒå±€å±‚"""
    
    def __init__(self):
        self.content_layer = []  # çº¯æ–‡æœ¬å†…å®¹
        self.format_layer = []   # æ ¼å¼ä¿¡æ¯
        self.layout_layer = []   # å¸ƒå±€ä¿¡æ¯
        self.anchors = {}        # é”šç‚¹æ˜ å°„
    
    def parse_document(self, doc_path: str) -> Dict[str, Any]:
        """è§£æWordæ–‡æ¡£ï¼Œæå–ä¸‰å±‚ä¿¡æ¯"""
        try:
            doc = Document(doc_path)
            
            # åˆå§‹åŒ–è§£æç»“æœ
            result = {
                'content_layer': [],
                'format_layer': [],
                'layout_layer': [],
                'anchors': {},
                'metadata': {
                    'total_paragraphs': 0,
                    'total_tables': 0,
                    'total_images': 0,
                    'total_pages': 0
                }
            }
            
            # è§£ææ®µè½
            for i, paragraph in enumerate(doc.paragraphs):
                if paragraph.text.strip():
                    # å†…å®¹å±‚ï¼šçº¯æ–‡æœ¬
                    content_info = {
                        'id': f'para_{i}',
                        'text': paragraph.text.strip(),
                        'type': 'paragraph'
                    }
                    result['content_layer'].append(content_info)
                    
                    # æ ¼å¼å±‚ï¼šæ ·å¼ä¿¡æ¯
                    format_info = {
                        'id': f'para_{i}',
                        'style': paragraph.style.name,
                        'alignment': str(paragraph.alignment),
                        'runs': []
                    }
                    
                    for run in paragraph.runs:
                        run_info = {
                            'text': run.text,
                            'bold': run.bold,
                            'italic': run.italic,
                            'underline': run.underline,
                            'font_name': run.font.name,
                            'font_size': run.font.size.pt if run.font.size else None
                        }
                        format_info['runs'].append(run_info)
                    
                    result['format_layer'].append(format_info)
                    
                    # å¸ƒå±€å±‚ï¼šç»“æ„ä¿¡æ¯
                    layout_info = {
                        'id': f'para_{i}',
                        'is_heading': paragraph.style.name.startswith('Heading'),
                        'heading_level': self._get_heading_level(paragraph.style.name),
                        'page_break_before': paragraph._element.getparent().tag.endswith('pPr')
                    }
                    result['layout_layer'].append(layout_info)
                    
                    result['metadata']['total_paragraphs'] += 1
            
            # è§£æè¡¨æ ¼
            for i, table in enumerate(doc.tables):
                table_content = self._parse_table(table, i)
                result['content_layer'].extend(table_content['content'])
                result['format_layer'].extend(table_content['format'])
                result['layout_layer'].extend(table_content['layout'])
                result['metadata']['total_tables'] += 1
            
            # è§£æå›¾ç‰‡
            for i, rel in enumerate(doc.part.rels.values()):
                if "image" in rel.target_ref:
                    image_info = {
                        'id': f'img_{i}',
                        'type': 'image',
                        'target': rel.target_ref,
                        'content_type': rel.target_content_type
                    }
                    result['content_layer'].append(image_info)
                    result['metadata']['total_images'] += 1
            
            return result
            
        except Exception as e:
            st.error(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
            return None
    
    def _get_heading_level(self, style_name: str) -> int:
        """è·å–æ ‡é¢˜çº§åˆ«"""
        if 'Heading 1' in style_name:
            return 1
        elif 'Heading 2' in style_name:
            return 2
        elif 'Heading 3' in style_name:
            return 3
        elif 'Heading 4' in style_name:
            return 4
        elif 'Heading 5' in style_name:
            return 5
        elif 'Heading 6' in style_name:
            return 6
        return 0
    
    def _parse_table(self, table, table_index: int) -> Dict[str, List]:
        """è§£æè¡¨æ ¼ - ä¿®å¤é‡å¤é—®é¢˜"""
        content = []
        format_info = []
        layout_info = []
        
        # ä½¿ç”¨é›†åˆæ¥è·Ÿè¸ªå·²å¤„ç†çš„å•å…ƒæ ¼å†…å®¹ï¼Œé¿å…é‡å¤
        processed_cells = set()
        
        for row_idx, row in enumerate(table.rows):
            for col_idx, cell in enumerate(row.cells):
                cell_text = cell.text.strip()
                
                # è·³è¿‡ç©ºå•å…ƒæ ¼
                if not cell_text:
                    continue
                
                # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
                cell_key = f"{table_index}_{row_idx}_{col_idx}_{cell_text}"
                if cell_key in processed_cells:
                    continue
                
                processed_cells.add(cell_key)
                
                cell_id = f'table_{table_index}_row_{row_idx}_col_{col_idx}'
                
                # å†…å®¹å±‚
                content.append({
                    'id': cell_id,
                    'text': cell_text,
                    'type': 'table_cell',
                    'table_index': table_index,
                    'row': row_idx,
                    'col': col_idx
                })
                
                # æ ¼å¼å±‚
                format_info.append({
                    'id': cell_id,
                    'cell_style': 'table_cell',
                    'runs': []
                })
                
                # å¸ƒå±€å±‚
                layout_info.append({
                    'id': cell_id,
                    'type': 'table_cell',
                    'table_index': table_index,
                    'row': row_idx,
                    'col': col_idx
                })
        
        return {
            'content': content,
            'format': format_info,
            'layout': layout_info
        }

class SemanticTranslator:
    """è¯­ä¹‰å¢å¼ºç¿»è¯‘å™¨ - æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†ã€æœ¯è¯­é”å®šã€é£æ ¼æ¨¡ä»¿ã€ä¸“æœ‰åè¯ä¿æŠ¤"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.context_memory = {}  # ä¸Šä¸‹æ–‡è®°å¿†
        self.terminology = {}     # æœ¯è¯­é”å®š
        self.style_examples = {}  # é£æ ¼ç¤ºä¾‹
        self.proper_nouns = set()  # ä¸“æœ‰åè¯é›†åˆ
        self._init_proper_nouns()  # åˆå§‹åŒ–å¸¸è§ä¸“æœ‰åè¯
        
    def set_terminology(self, terms: Dict[str, str]):
        """è®¾ç½®æœ¯è¯­é”å®š"""
        self.terminology = terms
    
    def set_style_examples(self, examples: Dict[str, str]):
        """è®¾ç½®é£æ ¼ç¤ºä¾‹"""
        self.style_examples = examples
    
    def _init_proper_nouns(self):
        """åˆå§‹åŒ–å¸¸è§ä¸“æœ‰åè¯"""
        # æŠ€æœ¯å…¬å¸/å¹³å°
        tech_companies = [
            'GitHub', 'Google', 'Microsoft', 'Apple', 'Amazon', 'Facebook', 'Meta',
            'Twitter', 'LinkedIn', 'Instagram', 'YouTube', 'Netflix', 'Spotify',
            'OpenAI', 'Anthropic', 'Claude', 'ChatGPT', 'GPT', 'DALL-E',
            'Streamlit', 'Docker', 'Kubernetes', 'React', 'Vue', 'Angular',
            'Node.js', 'Python', 'JavaScript', 'TypeScript', 'Java', 'C++',
            'TensorFlow', 'PyTorch', 'Scikit-learn', 'Pandas', 'NumPy'
        ]
        
        # å¼€æºé¡¹ç›®
        open_source = [
            'Linux', 'Apache', 'Nginx', 'MySQL', 'PostgreSQL', 'MongoDB',
            'Redis', 'Elasticsearch', 'Kibana', 'Grafana', 'Prometheus',
            'Jenkins', 'GitLab', 'Bitbucket', 'Jira', 'Confluence'
        ]
        
        # åè®®å’Œæ ‡å‡†
        protocols = [
            'HTTP', 'HTTPS', 'FTP', 'SSH', 'SMTP', 'POP3', 'IMAP',
            'TCP', 'UDP', 'IP', 'DNS', 'SSL', 'TLS', 'OAuth', 'JWT',
            'REST', 'GraphQL', 'WebSocket', 'gRPC', 'JSON', 'XML', 'YAML'
        ]
        
        # å­¦æœ¯æœºæ„
        universities = [
            'MIT', 'Stanford', 'Harvard', 'Berkeley', 'CMU', 'Oxford',
            'Cambridge', 'Yale', 'Princeton', 'Caltech', 'UCLA', 'NYU'
        ]
        
        # æ·»åŠ åˆ°ä¸“æœ‰åè¯é›†åˆ
        all_proper_nouns = tech_companies + open_source + protocols + universities
        self.proper_nouns.update(all_proper_nouns)
    
    def add_proper_nouns(self, nouns: List[str]):
        """æ·»åŠ è‡ªå®šä¹‰ä¸“æœ‰åè¯"""
        self.proper_nouns.update(nouns)
    
    def _protect_proper_nouns(self, text: str) -> Tuple[str, Dict[str, str]]:
        """ä¿æŠ¤ä¸“æœ‰åè¯ï¼Œè¿”å›æ›¿æ¢åçš„æ–‡æœ¬å’Œæ˜ å°„è¡¨"""
        protected_text = text
        noun_mapping = {}
        
        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿ä¸“æœ‰åè¯
        sorted_nouns = sorted(self.proper_nouns, key=len, reverse=True)
        
        for noun in sorted_nouns:
            if noun in protected_text:
                # åˆ›å»ºå ä½ç¬¦
                placeholder = f"__PROPER_NOUN_{len(noun_mapping)}__"
                noun_mapping[placeholder] = noun
                protected_text = protected_text.replace(noun, placeholder)
        
        return protected_text, noun_mapping
    
    def _identify_special_names_with_ai(self, text: str) -> List[str]:
        """ä½¿ç”¨OpenAIæ™ºèƒ½è¯†åˆ«ç‰¹æ®Šåç§°ï¼ˆGitHubåº“åã€é¡¹ç›®åç­‰ï¼‰"""
        try:
            # æ„å»ºè¯†åˆ«æç¤º
            prompt = f"""
è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œè¯†åˆ«å‡ºæ‰€æœ‰åº”è¯¥ä¿æŒä¸å˜çš„ç‰¹æ®Šåç§°ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

1. GitHubåº“åï¼ˆå¦‚ï¼šnaiveHobo/InvoiceNet, microsoft/TypeScriptï¼‰
2. é¡¹ç›®åç§°ï¼ˆå¦‚ï¼šReact, Vue.js, Angularï¼‰
3. æŠ€æœ¯æ¡†æ¶åï¼ˆå¦‚ï¼šTensorFlow, PyTorch, Scikit-learnï¼‰
4. å…¬å¸/ç»„ç»‡åï¼ˆå¦‚ï¼šGoogle, Microsoft, OpenAIï¼‰
5. äº§å“åç§°ï¼ˆå¦‚ï¼šChatGPT, GitHub Copilotï¼‰
6. åè®®/æ ‡å‡†ï¼ˆå¦‚ï¼šHTTP, JSON, XMLï¼‰
7. ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ï¼šPython, JavaScript, TypeScriptï¼‰

æ–‡æœ¬å†…å®¹ï¼š{text}

è¯·åªè¿”å›è¯†åˆ«å‡ºçš„ç‰¹æ®Šåç§°ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚
å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ç‰¹æ®Šåç§°ï¼Œè¯·è¿”å›ç©ºè¡Œã€‚
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è¯†åˆ«æŠ€æœ¯æ–‡æ¡£ä¸­çš„ç‰¹æ®Šåç§°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            # è§£æè¿”å›çš„ç‰¹æ®Šåç§°
            identified_names = []
            response_text = response.choices[0].message.content.strip()
            
            if response_text:
                for line in response_text.split('\n'):
                    name = line.strip()
                    if name and name in text:
                        identified_names.append(name)
            
            return identified_names
            
        except Exception as e:
            print(f"AIè¯†åˆ«ç‰¹æ®Šåç§°å¤±è´¥: {str(e)}")
            return []
    
    def _protect_special_names_with_ai(self, text: str) -> Tuple[str, Dict[str, str]]:
        """ä½¿ç”¨AIæ™ºèƒ½ä¿æŠ¤ç‰¹æ®Šåç§°"""
        try:
            # ä½¿ç”¨AIè¯†åˆ«ç‰¹æ®Šåç§°
            special_names = self._identify_special_names_with_ai(text)
            
            protected_text = text
            noun_mapping = {}
            
            # ä¿æŠ¤AIè¯†åˆ«çš„ç‰¹æ®Šåç§°
            for name in special_names:
                if name in protected_text:
                    placeholder = f"__SPECIAL_NAME_{len(noun_mapping)}__"
                    noun_mapping[placeholder] = name
                    protected_text = protected_text.replace(name, placeholder)
            
            return protected_text, noun_mapping
            
        except Exception as e:
            print(f"AIä¿æŠ¤ç‰¹æ®Šåç§°å¤±è´¥: {str(e)}")
            return text, {}
    
    def _restore_proper_nouns(self, text: str, noun_mapping: Dict[str, str]) -> str:
        """æ¢å¤ä¸“æœ‰åè¯"""
        restored_text = text
        for placeholder, noun in noun_mapping.items():
            restored_text = restored_text.replace(placeholder, noun)
        return restored_text
    
    def translate_with_context(self, content_items: List[Dict], target_lang: str) -> List[Dict]:
        """å¸¦ä¸Šä¸‹æ–‡çš„ç¿»è¯‘ - ä¿®å¤é‡å¤å†…å®¹é—®é¢˜"""
        try:
            openai.api_key = self.api_key
            
            # æ„å»ºä¸Šä¸‹æ–‡è®°å¿†
            context_prompt = self._build_context_prompt(content_items, target_lang)
            
            # æ‰¹é‡ç¿»è¯‘ï¼Œé¿å…é‡å¤
            translated_items = []
            processed_texts = {}  # ç”¨äºé¿å…é‡å¤ç¿»è¯‘ç›¸åŒå†…å®¹
            
            for item in content_items:
                if item['type'] == 'paragraph':
                    # æ£€æŸ¥æ˜¯å¦å·²ç»ç¿»è¯‘è¿‡ç›¸åŒå†…å®¹
                    text_key = item['text'].strip()
                    if text_key in processed_texts:
                        translated_text = processed_texts[text_key]
                    else:
                        translated_text = self._translate_paragraph(item, context_prompt, target_lang)
                        processed_texts[text_key] = translated_text
                    
                    translated_items.append({
                        **item,
                        'translated_text': translated_text
                    })
                    
                elif item['type'] == 'table_cell':
                    # è¡¨æ ¼å•å…ƒæ ¼ç‰¹æ®Šå¤„ç†ï¼Œé¿å…é‡å¤ç¿»è¯‘
                    text_key = f"table_{item.get('table_index', 0)}_{item.get('row', 0)}_{item.get('col', 0)}_{item['text'].strip()}"
                    if text_key in processed_texts:
                        translated_text = processed_texts[text_key]
                    else:
                        translated_text = self._translate_table_cell(item, context_prompt, target_lang)
                        processed_texts[text_key] = translated_text
                    
                    translated_items.append({
                        **item,
                        'translated_text': translated_text
                    })
                else:
                    translated_items.append(item)
            
            return translated_items
            
        except Exception as e:
            st.error(f"è¯­ä¹‰ç¿»è¯‘å¤±è´¥: {str(e)}")
            return content_items
    
    def _build_context_prompt(self, content_items: List[Dict], target_lang: str) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æç¤º"""
        # æ”¶é›†æ–‡æ¡£ä¸Šä¸‹æ–‡
        context_texts = []
        for item in content_items[:10]:  # å–å‰10ä¸ªæ®µè½ä½œä¸ºä¸Šä¸‹æ–‡
            if item['type'] in ['paragraph', 'table_cell']:
                context_texts.append(item['text'])
        
        context = " ".join(context_texts)
        
        # æ„å»ºæœ¯è¯­æç¤º
        term_prompt = ""
        if self.terminology:
            term_prompt = f"\næœ¯è¯­é”å®šï¼š{json.dumps(self.terminology, ensure_ascii=False)}"
        
        # æ„å»ºé£æ ¼æç¤º
        style_prompt = ""
        if self.style_examples:
            style_prompt = f"\né£æ ¼ç¤ºä¾‹ï¼š{json.dumps(self.style_examples, ensure_ascii=False)}"
        
        return f"""
        ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼š{context}
        {term_prompt}
        {style_prompt}
        
        è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­ä¸€è‡´æ€§å’Œæ–‡æ¡£é£æ ¼ã€‚
        """
    
    def _translate_paragraph(self, item: Dict, context: str, target_lang: str) -> str:
        """ç¿»è¯‘æ®µè½ - å¸¦AIæ™ºèƒ½ä¸“æœ‰åè¯ä¿æŠ¤"""
        try:
            original_text = item['text']
            
            # ä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«å’Œä¿æŠ¤ç‰¹æ®Šåç§°
            protected_text, ai_noun_mapping = self._protect_special_names_with_ai(original_text)
            
            # åŒæ—¶ä½¿ç”¨ä¼ ç»Ÿä¸“æœ‰åè¯ä¿æŠ¤
            protected_text, traditional_noun_mapping = self._protect_proper_nouns(protected_text)
            
            # åˆå¹¶æ˜ å°„è¡¨
            combined_mapping = {**ai_noun_mapping, **traditional_noun_mapping}
            
            # æ„å»ºç¿»è¯‘æç¤ºï¼Œå¼ºè°ƒä¸è¦ç¿»è¯‘ç‰¹æ®Šåç§°
            special_name_instruction = ""
            if combined_mapping:
                protected_names = list(combined_mapping.values())
                special_name_instruction = f"\né‡è¦ï¼šè¯·ä¿æŒä»¥ä¸‹ç‰¹æ®Šåç§°ä¸å˜ï¼š{', '.join(protected_names)}"
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": f"You are a professional document translator. {context}{special_name_instruction}"},
                    {"role": "user", "content": f"Translate this paragraph to {target_lang}: {protected_text}"}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            translated_text = response.choices[0].message.content
            
            # æ¢å¤æ‰€æœ‰ç‰¹æ®Šåç§°
            final_text = self._restore_proper_nouns(translated_text, combined_mapping)
            
            return final_text
        except Exception as e:
            st.warning(f"æ®µè½ç¿»è¯‘å¤±è´¥: {str(e)}")
            return item['text']
    
    def _translate_table_cell(self, item: Dict, context: str, target_lang: str) -> str:
        """ç¿»è¯‘è¡¨æ ¼å•å…ƒæ ¼ - å¸¦AIæ™ºèƒ½ä¸“æœ‰åè¯ä¿æŠ¤"""
        try:
            original_text = item['text']
            
            # ä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«å’Œä¿æŠ¤ç‰¹æ®Šåç§°
            protected_text, ai_noun_mapping = self._protect_special_names_with_ai(original_text)
            
            # åŒæ—¶ä½¿ç”¨ä¼ ç»Ÿä¸“æœ‰åè¯ä¿æŠ¤
            protected_text, traditional_noun_mapping = self._protect_proper_nouns(protected_text)
            
            # åˆå¹¶æ˜ å°„è¡¨
            combined_mapping = {**ai_noun_mapping, **traditional_noun_mapping}
            
            # æ„å»ºç¿»è¯‘æç¤ºï¼Œå¼ºè°ƒä¸è¦ç¿»è¯‘ç‰¹æ®Šåç§°
            special_name_instruction = ""
            if combined_mapping:
                protected_names = list(combined_mapping.values())
                special_name_instruction = f"\né‡è¦ï¼šè¯·ä¿æŒä»¥ä¸‹ç‰¹æ®Šåç§°ä¸å˜ï¼š{', '.join(protected_names)}"
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": f"You are a professional document translator. {context}{special_name_instruction}"},
                    {"role": "user", "content": f"Translate this table cell content to {target_lang}: {protected_text}"}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            translated_text = response.choices[0].message.content
            
            # æ¢å¤æ‰€æœ‰ç‰¹æ®Šåç§°
            final_text = self._restore_proper_nouns(translated_text, combined_mapping)
            
            return final_text
        except Exception as e:
            st.warning(f"è¡¨æ ¼å•å…ƒæ ¼ç¿»è¯‘å¤±è´¥: {str(e)}")
            return item['text']

class SmartReconstructor:
    """æ ¼å¼æ™ºèƒ½é‡å»ºå™¨ - åˆ©ç”¨é”šç‚¹æ˜ å°„é‡ç»„æ–‡æ¡£"""
    
    def __init__(self):
        self.anchors = {}
        self.format_preservation = True
    
    def reconstruct_document(self, original_doc_path: str, translated_content: List[Dict], 
                           format_layer: List[Dict], layout_layer: List[Dict], 
                           output_path: str) -> bool:
        """é‡å»ºæ–‡æ¡£"""
        try:
            # åŠ è½½åŸæ–‡æ¡£
            doc = Document(original_doc_path)
            
            # åˆ›å»ºç¿»è¯‘æ˜ å°„
            translation_map = {item['id']: item['translated_text'] for item in translated_content 
                              if 'translated_text' in item}
            
            # é‡å»ºæ®µè½
            self._reconstruct_paragraphs(doc, translation_map, format_layer)
            
            # é‡å»ºè¡¨æ ¼
            self._reconstruct_tables(doc, translation_map, format_layer)
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(output_path)
            return True
            
        except Exception as e:
            st.error(f"æ–‡æ¡£é‡å»ºå¤±è´¥: {str(e)}")
            return False
    
    def _reconstruct_paragraphs(self, doc: Document, translation_map: Dict, format_layer: List[Dict]):
        """é‡å»ºæ®µè½"""
        for i, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():
                para_id = f'para_{i}'
                if para_id in translation_map:
                    # æ™ºèƒ½å¤„ç†ç¿»è¯‘é•¿åº¦å˜åŒ–
                    original_text = paragraph.text
                    translated_text = translation_map[para_id]
                    
                    # å¦‚æœç¿»è¯‘é•¿åº¦å˜åŒ–ä¸å¤§ï¼Œç›´æ¥æ›¿æ¢
                    if abs(len(translated_text) - len(original_text)) / len(original_text) < 0.5:
                        paragraph.text = translated_text
                    else:
                        # é•¿åº¦å˜åŒ–å¤§ï¼Œéœ€è¦æ™ºèƒ½è°ƒæ•´
                        self._smart_text_replacement(paragraph, translated_text)
    
    def _reconstruct_tables(self, doc: Document, translation_map: Dict, format_layer: List[Dict]):
        """é‡å»ºè¡¨æ ¼ - ä¿®å¤é‡å¤é—®é¢˜"""
        table_index = 0
        for table in doc.tables:
            for row_idx, row in enumerate(table.rows):
                for col_idx, cell in enumerate(row.cells):
                    # åˆ›å»ºå•å…ƒæ ¼ID
                    cell_id = f'table_{table_index}_row_{row_idx}_col_{col_idx}'
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„ç¿»è¯‘
                    if cell_id in translation_map:
                        translated_text = translation_map[cell_id]
                        if translated_text and translated_text.strip():
                            # æ™ºèƒ½å¤„ç†ç¿»è¯‘é•¿åº¦å˜åŒ–
                            original_text = cell.text.strip()
                            if original_text:
                                # å¦‚æœç¿»è¯‘é•¿åº¦å˜åŒ–ä¸å¤§ï¼Œç›´æ¥æ›¿æ¢
                                if abs(len(translated_text) - len(original_text)) / len(original_text) < 0.5:
                                    cell.text = translated_text
                                else:
                                    # é•¿åº¦å˜åŒ–å¤§ï¼Œéœ€è¦æ™ºèƒ½è°ƒæ•´
                                    self._smart_cell_replacement(cell, translated_text)
            table_index += 1
    
    def _smart_cell_replacement(self, cell, translated_text: str):
        """æ™ºèƒ½å•å…ƒæ ¼æ–‡æœ¬æ›¿æ¢"""
        # æ¸…ç©ºå•å…ƒæ ¼å†…å®¹
        cell.text = ""
        # æ·»åŠ ç¿»è¯‘æ–‡æœ¬
        cell.text = translated_text
    
    def _smart_text_replacement(self, paragraph, translated_text: str):
        """æ™ºèƒ½æ–‡æœ¬æ›¿æ¢ï¼Œå¤„ç†é•¿åº¦å˜åŒ–"""
        # ä¿æŒåŸæœ‰çš„runç»“æ„ï¼Œä½†è°ƒæ•´å†…å®¹
        runs = paragraph.runs
        if runs:
            # å°†ç¿»è¯‘æ–‡æœ¬åˆ†é…ç»™ç¬¬ä¸€ä¸ªrun
            runs[0].text = translated_text
            # æ¸…ç©ºå…¶ä»–run
            for run in runs[1:]:
                run.text = ""

class FormatCorrector:
    """æ ¼å¼çº é”™æ¨¡å— - æ£€æµ‹å’Œä¿®å¤æ’ç‰ˆé—®é¢˜"""
    
    def __init__(self):
        self.correction_rules = []
    
    def detect_format_issues(self, doc_path: str) -> List[Dict]:
        """æ£€æµ‹æ ¼å¼é—®é¢˜"""
        issues = []
        try:
            doc = Document(doc_path)
            
            # æ£€æµ‹è¡¨æ ¼æº¢å‡º
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if len(cell.text) > 100:  # å•å…ƒæ ¼å†…å®¹è¿‡é•¿
                            issues.append({
                                'type': 'table_overflow',
                                'location': 'table',
                                'description': 'è¡¨æ ¼å•å…ƒæ ¼å†…å®¹è¿‡é•¿',
                                'suggestion': 'å»ºè®®æ‹†åˆ†é•¿æ–‡æœ¬'
                            })
            
            # æ£€æµ‹æ ‡é¢˜æ ¼å¼
            for paragraph in doc.paragraphs:
                if paragraph.style.name.startswith('Heading'):
                    if not paragraph.text.strip():
                        issues.append({
                            'type': 'empty_heading',
                            'location': 'paragraph',
                            'description': 'ç©ºæ ‡é¢˜',
                            'suggestion': 'å»ºè®®æ·»åŠ æ ‡é¢˜å†…å®¹æˆ–åˆ é™¤ç©ºæ ‡é¢˜'
                        })
            
            return issues
            
        except Exception as e:
            st.error(f"æ ¼å¼æ£€æµ‹å¤±è´¥: {str(e)}")
            return []
    
    def auto_fix_issues(self, doc_path: str, issues: List[Dict]) -> bool:
        """è‡ªåŠ¨ä¿®å¤æ ¼å¼é—®é¢˜"""
        try:
            doc = Document(doc_path)
            
            for issue in issues:
                if issue['type'] == 'table_overflow':
                    # è‡ªåŠ¨æ‹†åˆ†é•¿æ–‡æœ¬
                    self._fix_table_overflow(doc)
                elif issue['type'] == 'empty_heading':
                    # åˆ é™¤ç©ºæ ‡é¢˜
                    self._fix_empty_headings(doc)
            
            doc.save(doc_path)
            return True
            
        except Exception as e:
            st.error(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}")
            return False
    
    def _fix_table_overflow(self, doc: Document):
        """ä¿®å¤è¡¨æ ¼æº¢å‡º"""
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if len(cell.text) > 100:
                        # æ‹†åˆ†é•¿æ–‡æœ¬
                        text = cell.text
                        if len(text) > 100:
                            # åœ¨åˆé€‚çš„ä½ç½®æ‹†åˆ†
                            split_point = text.rfind(' ', 0, 100)
                            if split_point > 0:
                                cell.text = text[:split_point] + '\n' + text[split_point+1:]
    
    def _fix_empty_headings(self, doc: Document):
        """ä¿®å¤ç©ºæ ‡é¢˜"""
        paragraphs_to_remove = []
        for paragraph in doc.paragraphs:
            if paragraph.style.name.startswith('Heading') and not paragraph.text.strip():
                paragraphs_to_remove.append(paragraph)
        
        for paragraph in paragraphs_to_remove:
            p = paragraph._element
            p.getparent().remove(p)

class DualViewEditor:
    """åŒè§†å›¾ç¼–è¾‘å™¨ - å·¦å³å¯¹æ¯”æ˜¾ç¤º"""
    
    def __init__(self):
        self.original_content = []
        self.translated_content = []
    
    def display_dual_view(self, original_items: List[Dict], translated_items: List[Dict]):
        """æ˜¾ç¤ºåŒè§†å›¾ - ä¿®å¤é‡å¤å†…å®¹é—®é¢˜"""
        st.subheader("ğŸ“– åŒè§†å›¾ç¼–è¾‘å™¨")
        
        # å»é‡å¤„ç†
        original_unique = self._deduplicate_items(original_items)
        translated_unique = self._deduplicate_items(translated_items)
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ“„ åŸæ–‡")
            self._display_content(original_unique, "original")
        
        with col2:
            st.markdown("### ğŸŒ è¯‘æ–‡")
            self._display_content(translated_unique, "translated")
    
    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        """å»é‡å¤„ç†ï¼Œé¿å…é‡å¤æ˜¾ç¤º"""
        seen_texts = set()
        unique_items = []
        
        for item in items:
            text_key = item.get('text', '').strip()
            if text_key and text_key not in seen_texts:
                seen_texts.add(text_key)
                unique_items.append(item)
            elif not text_key:  # ä¿ç•™ç©ºå†…å®¹é¡¹
                unique_items.append(item)
        
        return unique_items
    
    def _display_content(self, items: List[Dict], view_type: str):
        """æ˜¾ç¤ºå†…å®¹ - ä¼˜åŒ–æ˜¾ç¤ºé€»è¾‘"""
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
        paragraphs = [item for item in items if item['type'] == 'paragraph']
        table_cells = [item for item in items if item['type'] == 'table_cell']
        
        # æ˜¾ç¤ºæ®µè½
        if paragraphs:
            st.markdown("**ğŸ“ æ®µè½å†…å®¹:**")
            for i, item in enumerate(paragraphs):
                if item['text'].strip():  # åªæ˜¾ç¤ºéç©ºæ®µè½
                    if view_type == "translated" and 'translated_text' in item:
                        st.text_area(f"æ®µè½ {i+1}", value=item['translated_text'], height=80, key=f"para_{view_type}_{i}")
                    else:
                        st.text_area(f"æ®µè½ {i+1}", value=item['text'], height=80, key=f"para_{view_type}_{i}")
        
        # æ˜¾ç¤ºè¡¨æ ¼å•å…ƒæ ¼ - ä¿®å¤é‡å¤é—®é¢˜
        if table_cells:
            st.markdown("**ğŸ“Š è¡¨æ ¼å†…å®¹:**")
            # æŒ‰è¡¨æ ¼åˆ†ç»„
            tables = {}
            for item in table_cells:
                table_idx = item.get('table_index', 0)
                if table_idx not in tables:
                    tables[table_idx] = []
                tables[table_idx].append(item)
            
            for table_idx, cells in tables.items():
                st.markdown(f"**è¡¨æ ¼ {table_idx + 1}:**")
                
                # å»é‡å¤„ç†ï¼šä½¿ç”¨é›†åˆè·Ÿè¸ªå·²æ˜¾ç¤ºçš„å•å…ƒæ ¼
                displayed_cells = set()
                
                # æŒ‰è¡Œåˆ—ç»„ç»‡
                rows = {}
                for cell in cells:
                    row = cell.get('row', 0)
                    col = cell.get('col', 0)
                    cell_text = cell['text'].strip()
                    
                    # è·³è¿‡ç©ºå•å…ƒæ ¼å’Œé‡å¤å†…å®¹
                    if not cell_text or cell_text in displayed_cells:
                        continue
                    
                    displayed_cells.add(cell_text)
                    
                    if row not in rows:
                        rows[row] = {}
                    rows[row][col] = cell
                
                # æ˜¾ç¤ºè¡¨æ ¼å†…å®¹
                for row_idx in sorted(rows.keys()):
                    cols = rows[row_idx]
                    if cols:  # åªæ˜¾ç¤ºæœ‰å†…å®¹çš„è¡Œ
                        st.markdown(f"**ç¬¬ {row_idx + 1} è¡Œ:**")
                        for col_idx in sorted(cols.keys()):
                            cell = cols[col_idx]
                            if cell['text'].strip():  # åªæ˜¾ç¤ºéç©ºå•å…ƒæ ¼
                                if view_type == "translated" and 'translated_text' in cell:
                                    st.text_input(f"åˆ—{col_idx+1}", value=cell['translated_text'], key=f"cell_{view_type}_{table_idx}_{row_idx}_{col_idx}")
                                else:
                                    st.text_input(f"åˆ—{col_idx+1}", value=cell['text'], key=f"cell_{view_type}_{table_idx}_{row_idx}_{col_idx}")

# ä¸»åº”ç”¨ç±»
class SmartDocumentTranslator:
    """æ™ºèƒ½æ–‡æ¡£ç¿»è¯‘ä¸æ ¼å¼ä¿çœŸç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        self.parser = StructuralParser()
        self.translator = None
        self.reconstructor = SmartReconstructor()
        self.corrector = FormatCorrector()
        self.editor = DualViewEditor()
    
    def set_translator(self, api_key: str):
        """è®¾ç½®ç¿»è¯‘å™¨"""
        self.translator = SemanticTranslator(api_key)
    
    def process_document(self, doc_path: str, target_lang: str, output_path: str) -> bool:
        """å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹"""
        try:
            # 1. ç»“æ„åˆ†å±‚è§£æ
            st.info("ğŸ” æ­£åœ¨è¿›è¡Œç»“æ„åˆ†å±‚è§£æ...")
            parsed_doc = self.parser.parse_document(doc_path)
            if not parsed_doc:
                return False
            
            # 2. è¯­ä¹‰å¢å¼ºç¿»è¯‘
            st.info("ğŸ¤– æ­£åœ¨è¿›è¡Œè¯­ä¹‰å¢å¼ºç¿»è¯‘...")
            if not self.translator:
                st.error("è¯·å…ˆè®¾ç½®ç¿»è¯‘å™¨")
                return False
            
            translated_content = self.translator.translate_with_context(
                parsed_doc['content_layer'], target_lang
            )
            
            # 3. æ ¼å¼æ™ºèƒ½é‡å»º
            st.info("ğŸ”§ æ­£åœ¨è¿›è¡Œæ ¼å¼æ™ºèƒ½é‡å»º...")
            success = self.reconstructor.reconstruct_document(
                doc_path, translated_content, 
                parsed_doc['format_layer'], parsed_doc['layout_layer'], 
                output_path
            )
            
            if success:
                # 4. æ ¼å¼çº é”™
                st.info("ğŸ” æ­£åœ¨è¿›è¡Œæ ¼å¼çº é”™...")
                issues = self.corrector.detect_format_issues(output_path)
                if issues:
                    st.warning(f"å‘ç° {len(issues)} ä¸ªæ ¼å¼é—®é¢˜ï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®å¤...")
                    self.corrector.auto_fix_issues(output_path, issues)
                
                return True
            
            return False
            
        except Exception as e:
            st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
            return False
